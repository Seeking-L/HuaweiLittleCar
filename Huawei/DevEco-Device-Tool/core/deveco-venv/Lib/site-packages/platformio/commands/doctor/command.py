# pylint: disable=too-many-arguments, (too-many-locals
import json
import click
import os
import re
import shutil
import collections
from platformio.util import get_systype
from os.path import (isfile, join, isdir, dirname, realpath, basename, exists)
from platformio.compat import dump_json_to_unicode
from platformio.helpers import get_installed_core_path, get_upload_tool_zip_path, get_config_json_path, get_suite_json_path
from platformio.commands.doctor.helpers import get_kernel_config_file_path, print_tools_list, update_prebuilts_download_status
from platformio.commands.doctor.helpers import search_compiler_dir, check_compiler_executable, check_pip_deb_pkg
from platformio.commands.home.rpc.handlers.app import AppRPC
from platformio.project.config import ProjectConfig
from platformio.commands.exception import ToolFileNotFound,UnknownError,HpmToolsJsonPathError

CompilerCheckArgs = collections.namedtuple('CompilerCheckArgs',
                                           ['project_dir', 'pre_compile', 'compiler_name', 'toolchain_dir',
                                            'dependencies', 'tools', 'hpm_tools'])


@click.group(short_help='DevEco Enviroment Doctor')
def cli():
    pass


@cli.command(
    'check',
    context_settings=dict(ignore_unknown_options=True),
    short_help='Check whether the dependent tools are ready',
)
@click.option(
    '-d',
    '--project-dir',
    default=os.getcwd,
    type=click.Path(
        exists=True, file_okay=False, dir_okay=True, writable=True, resolve_path=True
    ),
)
@click.option(
    '-t',
    '--toolchain-dir',
    type=click.Path(
        file_okay=False, dir_okay=True, writable=True, resolve_path=True
    ),
)
@click.option('-s', '--ohos-ver')
@click.option('-k', '--ohos-kernel', default='lite')
@click.option('-p', '--product', required=True)
@click.option('--tool', multiple=True)
@click.option('--from-cache', is_flag=True)
@click.option('--json-output', is_flag=True)
@click.option('--print-range', default="All", type=click.Choice(['All', 'Build', 'Upload']))
def doctor_check(**kwargs):
    project_dir = kwargs.get('project_dir')
    toolchain_dir = kwargs.get('toolchain_dir')
    ohos_ver = kwargs.get('ohos_ver')
    ohos_kernel = kwargs.get('ohos_kernel')
    product = kwargs.get('product')
    tool = kwargs.get('tool')
    from_cache = kwargs.get('from_cache')
    json_output = kwargs.get('json_output')

    if not json_output:
        click.secho(f'Checking env dependencies for product: {product}...', fg='blue')

    ohos_ver = re.sub(r'(\d+)\..+', lambda match: f'{match.group(1)}.x', ohos_ver) if ohos_ver else None
    cache_status_path = join(project_dir, '.deveco', 'status', f'{re.sub("[@/]", ".", product)}')
    toolchain_dir = toolchain_dir if toolchain_dir else _get_toolchain_dir()
    tool_status = {'all_valid': False, 'build_valid': False, 'upload_valid': False, 'hpm_valid': False,
                   'tools': {'pip': {}, 'deb': {}, 'prebuilt': {}, 'hpm': {},
                             'compiler': {}, 'upload': {}, 'suite_tool':{}}}

    pre_tool_status = {}
    if isfile(cache_status_path):
        pre_tool_status = json.load(open(cache_status_path, 'r'))
        if from_cache:
            tool_status = pre_tool_status

    use_cache_for_pip_deb = from_cache and isfile(cache_status_path)
    check_tools = tool_status.get('tools', {})

    dependencies, upload_tools, profile_tools, have_suite = _get_meta_datas(project_dir, product)

    # hpm product (bundle) startswith @ and hb product does not
    is_hpm_product = product.startswith('@')
    is_lite = 'lite' in ohos_kernel if ohos_kernel else product.split('@').pop() != 'built-in'
    compiler_tool = {}

    # check hpm tools firstly. There may be compiler tools that can be used for compiler checks.
    check_hpm_tools, hpm_ok = _check_profile_tools(profile_tools)

    # parse config.gni to get compiler of mini/small system
    if is_lite:
        compiler_name = _get_compiler_name(project_dir, product, ohos_ver, dependencies)
        if compiler_name:
            check_args = CompilerCheckArgs(project_dir,
                pre_tool_status.get('tools', {}).get('compiler', {}),
                compiler_name, toolchain_dir, dependencies, tool, check_hpm_tools)
            compiler_tool = _get_lite_compiler(check_args)

    # check build tools
    check_build_args = dict(
        dependencies=dependencies, ohos_ver=ohos_ver, is_hpm_product=is_hpm_product, is_lite=is_lite,
        product=product, compiler_tool=compiler_tool, pre_tool_status=pre_tool_status,
        toolchain_dir=toolchain_dir, use_cache_for_pip_deb=use_cache_for_pip_deb,
        check_tools=check_tools)
    check_tools, build_valid = _check_build_tool(check_build_args)

    # check upload tools
    check_upload_args = dict(tool=tool, use_cache=use_cache_for_pip_deb, have_suite=have_suite)
    upload_ok = _check_upload_tool(
        check_tools, upload_tools, pre_tool_status, check_upload_args)

    check_tools.update({'hpm': check_hpm_tools})

    tool_status = _save_cache_file(cache_status_path, build_valid, upload_ok, hpm_ok, check_tools)

    _print_result(json_output, tool_status, kwargs.get('print_range'))


def _get_toolchain_dir():
    state = AppRPC.load_state()
    return state.get('storage', {}).get('toolchainDir', '')


def _print_result(json_output, tool_status, print_range):
    if json_output:
        click.echo(dump_json_to_unicode(tool_status))
    else:
        print_tools_list(tool_status, print_range)


def _get_meta_datas(project_dir, product):
    dependencies, upload_tools, profile_tools = {},{},{}
    have_suite = False
    try:
        # build tool
        build_data_path = os.path.join(get_installed_core_path(), 'core', 'tool_utils', 'dependent_tools.json')
        dependencies = json.load(open(build_data_path, 'rt'))
        # upload tool
        upload_data_path = os.path.join(get_installed_core_path(), 'core', 'tool_utils', 'upload_tools.json')
        tools_data = json.load(open(upload_data_path, 'rt'))

        product_board, have_suite = _get_product_board_suite(project_dir, product)
        upload_tools = tools_data.get(product_board, {})
        # profile
        chip_profile_path = realpath(join(project_dir, '.deveco', 'profile', 'bundle.json'))
        profile_tools = json.load(open(chip_profile_path, 'rt')).get('dependencies', {})
    except (json.JSONDecodeError, FileNotFoundError) as e:
        ToolFileNotFound(e)
    return dependencies, upload_tools, profile_tools, have_suite


def _get_product_board_suite(project_dir, product):
    board = ''
    have_suite = False
    deveco_ini = os.path.join(project_dir, '.deveco', 'deveco.ini')
    config = ProjectConfig(deveco_ini, parse_extra=False, expand_interpolations=False)
    framework = config.get('env', 'framework', [])
    for env in config.envs():
        product_current = config.get('env:' + env, 'board_frameworks.hb.build.product', '') \
            if 'hpm' not in framework else config.get("env:" + env, 'hpm_project_base_package', '')
        if product_current == product:
            board = config.get('env:' + env, 'board', '')
            if config.has_option(f'env:{env}', 'suite_burn_enable'):
                have_suite = True
            break

    return board, have_suite


def _save_cache_file(cache_status_path, build_valid, upload_ok, hpm_ok, check_tools):
    tool_status = {
        'all_valid': build_valid and upload_ok and hpm_ok,
        'build_valid': build_valid,
        'upload_valid': upload_ok,
        'hpm_valid': hpm_ok,
        'tools': check_tools
    }
    if not isdir(dirname(cache_status_path)):
        os.makedirs(dirname(cache_status_path))

    with os.fdopen(os.open(cache_status_path, os.O_RDWR | os.O_CREAT | os.O_TRUNC, 0o640), 'w') as setting_file:
        json.dump(tool_status, setting_file, indent=4)

    return tool_status


def _check_build_tool(check_build_args):
    dependencies = check_build_args.get('dependencies')
    ohos_ver = check_build_args.get('ohos_ver')
    is_hpm_product = check_build_args.get('is_hpm_product')
    is_lite = check_build_args.get('is_lite')
    product = check_build_args.get('product')
    compiler_tool = check_build_args.get('compiler_tool')
    pre_tool_status = check_build_args.get('pre_tool_status')
    toolchain_dir = check_build_args.get('toolchain_dir')
    use_cache_for_pip_deb = check_build_args.get('use_cache_for_pip_deb')
    check_tools = check_build_args.get('check_tools')

    build_valid = True
    for tools in dependencies.get("ohos_toolchain", []):
        if tools.get("version", "") != ohos_ver:
            continue
        specific_tools = tools.get('specific-tools', {})
        common = tools.get('common-tools', {})
        if is_hpm_product:
            common.pop('prebuilt', '')
        depend_tools = {
            'common': common,
            'level': specific_tools.get('standard' if not is_lite else 'lite', {}),
            'product': specific_tools.get(product, {})
        }
        check_tools, build_ok = _check_tool_status(
            compiler_tool,
            pre_tool_status.get('tools', {}),
            toolchain_dir,
            depend_tools,
            ohos_ver=ohos_ver,
            use_cache=use_cache_for_pip_deb
        )
        build_valid = build_ok
    return check_tools, build_valid


def _check_profile_tools(profile_tools):
    # load hpm_tools.json, it record downloaded tools used hpm.
    tools_status = {}
    hpm_tools_json = {}
    tool_ok = True
    hpm_tools_json_path = os.path.join(get_config_json_path(), 'hpm_tools.json')
    try:
        hpm_tools_json = json.load(open(hpm_tools_json_path, 'r'))
    except (json.JSONDecodeError, FileNotFoundError) as e:
        HpmToolsJsonPathError(e)
    except Exception as e:
        UnknownError(e)

    for hpm_tool, version in profile_tools.items():
        tool_info = hpm_tools_json.get(hpm_tool, {})
        record_version = tool_info.get('version', '')
        store_path = tool_info.get('store', '')
        if all([
            hpm_tool in hpm_tools_json,
            record_version == version,
            exists(store_path)
        ]):
            tool_info.update({'valid': True})
            tools_status.update({hpm_tool: tool_info})
        else:
            tool_ok = False
            tools_status.update({
                hpm_tool: {
                    'valid': False,
                    'version': version,
                    'user_dir': get_config_json_path()
                }
            })
            if hpm_tool in hpm_tools_json:
                del hpm_tools_json[hpm_tool]

    flags = os.O_RDWR | os.O_CREAT | os.O_TRUNC
    with os.fdopen(os.open(hpm_tools_json_path, flags, 0o640), "w") as fp:
        json.dump(hpm_tools_json, fp, indent=4)

    return tools_status, tool_ok


def _check_upload_tool(check_tools, upload_tools, pre_tool_status, check_upload_args):
    custom_tools = check_upload_args.get('tool')
    use_cache = check_upload_args.get('use_cache')
    have_suite = check_upload_args.get('have_suite')
    custom_tool_paths = {}
    for t in custom_tools:
        if t.count('@') > 1:
            first_index = t.find('@')
            last_index = t.rfind('@')
            custom_tool_paths[t[:first_index]] = dict(dest=t[first_index + 1:last_index], status=t[last_index + 1:])

    check_upload = {}
    upload_ok = False if not upload_tools and have_suite else True
    pre_upload = pre_tool_status.get('tools', {}).get('upload', {})
    for k, v in upload_tools.items():
        check_upload[k] = pre_upload.get(k, {})
        pre_version = pre_upload.get(k, {}).get('version', '')
        is_custom = pre_upload.get(k, {}).get('custom', False)
        is_set = all([
            not use_cache,
            not is_custom
        ])
        if k in custom_tool_paths:
            if custom_tool_paths.get(k, {}).get('dest', ''):
                custom_valid = True if custom_tool_paths.get(k, {}).get('status', '') == 'true' else False
                check_upload[k] = dict(version=v.get('version', ''), dest=custom_tool_paths.get(k, {}).get('dest', ''),
                                       exe_name=v.get('exe_name', ''), custom=True, valid=custom_valid)
            else:
                check_upload[k] = _set_download_upload(k, v)

        elif (pre_version and pre_version != v.get('version', '')) or is_set:
            check_upload[k] = _set_download_upload(k, v)

        if not check_upload.get(k, {}).get('valid', True):
            upload_ok = False

    check_tools['upload'] = check_upload

    pre_suite_tool = pre_tool_status.get('tools', {}).get('suite_tool', {})
    if pre_suite_tool:
        check_tools['suite_tool'] = pre_suite_tool
        upload_ok = upload_ok or pre_suite_tool.get('valid', False)

    return upload_ok


def _set_download_upload(key, value):
    tools_path = get_upload_tool_zip_path()
    store_path = os.path.join(tools_path, key)
    download_upload = dict(version=value.get('version', ''), store=store_path,
                            custom=False, zip_name=value.get('windows', ''), url=value.get('url', ''), 
                            sha256=value.get('sha256', ''), exe_name=value.get('exe_name', ''),
                            hpm_name=value.get('hpm_name', ''), hpm_dest=value.get('hpm_dest', ''))
    upload_path = os.path.join(store_path, value.get('windows', ''))
    if os.path.exists(upload_path):
        download_upload['valid'] = True
    else:
        download_upload['valid'] = False
    return download_upload


def _get_compiler_name(project_dir, product, ohos_ver, dependencies):
    for tools in dependencies.get("ohos_toolchain", []):
        if tools.get("version", "") != ohos_ver:
            continue
        specific_compiler = tools.get('specific-tools', {}).get(product, {}).get('compiler', '')
        if specific_compiler:
            return specific_compiler

    try:
        config_gni_path = get_kernel_config_file_path(project_dir, product)
        if not isfile(config_gni_path):
            return ''
    except Exception as e:  # pylint: disable=bare-except
        UnknownError(e)
        return ''

    compiler_name = ''
    with open(config_gni_path, 'r', encoding='utf-8') as config_file:
        file_text = config_file.read()
        toolchain_pattern = r'^\s*board_toolchain ?= ?"(.*)"'
        toolchain_list = re.findall(toolchain_pattern, file_text, re.M)
        prefix_pattern = r'^\s*board_toolchain_prefix ?= ?"(.*)"'
        prefix_list = re.findall(prefix_pattern, file_text, re.M)
        compiler_name = basename(prefix_list[0].strip()) + 'gcc' \
            if toolchain_list and toolchain_list[0].strip() and prefix_list else 'clang'

    # if no prefix, treat it as invalid compiler_name
    if not compiler_name or compiler_name == 'gcc' or (compiler_name == 'clang' and ohos_ver not in ['1.x', '2.x']):
        return ''
    return compiler_name


def _compiler_check_noncustom(project_dir, compiler_name, toolchain_dir, compiler_tool, hpm_tools):  
    # check hpm tools firstly
    for tool in hpm_tools:
        tool_name = tool.split('/').pop()
        storage_path = join(
            toolchain_dir,
            'compilers',
            tool_name
        )
        bin_path = search_compiler_dir(storage_path)
        if bin_path:
            return bin_path, storage_path

    # check sys env path
    bin_file_path = shutil.which(compiler_name)
    if bin_file_path:
        bin_path = storage_path = dirname(bin_file_path)
        return bin_path, storage_path

    # default path
    storage_path = join(
        toolchain_dir,
        'compilers',
        compiler_name,
        compiler_tool.get('version', '')
    )
    bin_path = join(storage_path, compiler_tool.get('environment_path', ''))
    return bin_path, storage_path


def _get_lite_compiler(check_args):
    project_dir = check_args.project_dir
    pre_compile = check_args.pre_compile
    compiler_name = check_args.compiler_name
    toolchain_dir = check_args.toolchain_dir
    dependencies = check_args.dependencies
    tools = check_args.tools
    hpm_tools = check_args.hpm_tools
    tool_paths = {t.split('@')[0]: t.split('@')[1] for t in tools if len(t.split('@')) == 3}
    compiler_tool = dependencies.get("compilers", {}).get('prebuilt', {}).get(compiler_name, {})
    storage_path = ''
    bin_path = ''
    is_custom_path = False
    if compiler_name in tool_paths.keys():
        is_custom_path = tool_paths[compiler_name] != ''
        bin_path = storage_path = tool_paths[compiler_name]
    elif pre_compile.get(compiler_name, {}).get('custom', False):
        is_custom_path = True
        bin_path = storage_path = pre_compile.get(compiler_name, {}).get('dest')

    if not is_custom_path:
        bin_path, storage_path = _compiler_check_noncustom(
            project_dir, compiler_name, toolchain_dir, compiler_tool, hpm_tools)

    compiler_tool['valid'] = check_compiler_executable(bin_path, compiler_name)
    compiler_tool['dest'] = bin_path
    compiler_tool['store'] = storage_path
    compiler_tool['custom'] = is_custom_path
    return {compiler_name: compiler_tool}


def _check_tool_status(compiler, pre_check_tools, toolchain_dir, depend_tools, **kwargs):
    ohos_ver = kwargs.get("ohos_ver")
    use_cache = kwargs.get("use_cache")
    all_ok = True
    tools = {'compiler': compiler, 'upload': {}, 'suite_tool': {}}
    for tool_type in ['pip', 'deb', 'prebuilt', 'inner']:
        specific_tools = dict(depend_tools.get('level', {}).get(tool_type, {}),
                              **depend_tools.get('product', {}).get(tool_type, {}))
        tools[tool_type] = dict(
            depend_tools.get('common', {}).get(tool_type, {}), **specific_tools
        )
        for tool, tool_data in tools.get(tool_type, {}).items():
            tool_dict = {
                'tool': tool,
                'tool_data': tool_data,
                'tool_type': tool_type,
                'pre_check_tools': pre_check_tools,
                'toolchain_dir': toolchain_dir,
                'tools': tools
            }
            all_ok = handle_check_tool_status(tool_dict, use_cache, ohos_ver, all_ok)

    for _, tool_data in tools.get('compiler', {}).items():
        all_ok = all_ok and tool_data.get('valid', False)

    return tools, all_ok


def handle_check_tool_status(tool_dict, use_cache, ohos_ver, all_ok):
    tool = tool_dict.get('tool')
    tool_data = tool_dict.get('tool_data')
    tool_type = tool_dict.get('tool_type')

    if tool_type in ['pip', 'deb']:
        if use_cache and tool in tool_dict.get('pre_check_tools').get(tool_type, {}):
            tool_data.update(tool_dict.get('pre_check_tools').get(tool_type, {}).get(tool, {}))
        else:
            tool_data['valid'] = check_pip_deb_pkg(tool, tool_type)
    elif tool_type == 'prebuilt':
        bin_file_path = shutil.which(tool)
        if bin_file_path:
            tool_data['valid'] = isfile(bin_file_path)
            tool_data['dest'] = dirname(bin_file_path)
        else:
            storage_path = realpath(join(
                tool_dict.get('toolchain_dir'),
                'building-tools',
                f'ohos_{ohos_ver}', tool,
                tool_data.get('version', '')
            )) if tool_dict.get('toolchain_dir') else ''
            bin_path = join(storage_path,
                            tool_data.get('environment_path', '')
                            )
            tool_data['valid'] = isdir(bin_path)
            tool_data['dest'] = bin_path
            tool_data['store'] = storage_path

        if tool in tool_dict.get('tools').get('compiler', {}):
            compiler_data = tool_dict.get('tools').get('compiler', {}).get(tool, {})
            if not compiler_data.get('valid', False) and \
                    not compiler_data.get('custom', False):
                compiler_data.update(tool_data)
            tool_data = compiler_data
    else:
        tool_data['valid'] = True
        tool_data['dest'] = realpath(
            join(get_installed_core_path(), tool_data.get('path', ''))
        )

    return all_ok and tool_data.get('valid')


@cli.command(
    'status',
    context_settings=dict(ignore_unknown_options=True),
    short_help='Get tools status of env',
)
@click.option(
    '-d',
    '--project-dir',
    default=os.getcwd,
    type=click.Path(
        exists=True, file_okay=False, dir_okay=True, writable=True, resolve_path=True
    ),
)
@click.option('-e', '--env', required=True)
@click.option('-t', '--target', required=True)
def doctor_status(project_dir, env, target):
    config = ProjectConfig.get_instance(
        os.path.join(project_dir, '.deveco', 'deveco.ini')
    )
    ohos_version = config.get('env', 'ohos_version', '')
    status = {
        'build_valid': ohos_version == '',
        'upload_valid': ohos_version == '',
        'is_linux': "linux" in get_systype(),
        'is_bash': True,
        'prebuilt_ok': True
    }
    tool_status = {}
    product_hb = config.get('env:' + env, 'board_frameworks.hb.build.product', '')
    product_hpm = config.get('env:' + env, 'hpm_project_base_package', '')
    product = product_hb or product_hpm
    if ohos_version and product:
        cache_status_path = join(project_dir, '.deveco', 'status', f'{re.sub("[@/]", ".", product)}')
        if isfile(cache_status_path):
            tool_status = json.load(open(cache_status_path, 'r'))
        else:
            status.update({
                'unchecked': {
                    'product': product,
                    'ohosVersion': ohos_version,
                    'ohosKernel': config.get('env:' + env, 'ohos_kernel', '')
                }
            })

    if target.strip('"').startswith('deveco: upload'):
        if tool_status:
            status.update({'upload_valid': tool_status.get('upload_valid', False)})
    elif "linux" in get_systype():
        if tool_status:
            status.update({'build_valid': tool_status.get('build_valid', False)})
        sh_path = shutil.which('sh')
        status.update({
            'is_bash': 'bash' in realpath(sh_path)
        })
        update_prebuilts_download_status(project_dir, status, config, env)

    click.echo(dump_json_to_unicode(status))


@cli.command("load_suite_json", short_help="load suite json config")
@click.option(
    '-d',
    '--project-dir',
    default=os.getcwd,
    type=click.Path(
        exists=True, file_okay=False, dir_okay=True, writable=True, resolve_path=True
    )
)
@click.option("--product", "-pd", help="the product name", required=True)
@click.option('-j', '--json-path', required=True)
def load_suite_json(project_dir, product, json_path):
    suite_json = get_suite_json_path(project_dir, product)
    status_path = join(project_dir, '.deveco', 'status', f'{re.sub("[@/]", ".", product)}')
    res = {"success": False, 'msg': "setSuiteJsonFail"}
    if exists(suite_json) and exists(status_path):
        try:
            suite_json_data = json.load(open(suite_json, 'r'))
            tool_status = json.load(open(status_path, 'r'))
        except json.decoder.JSONDecodeError:
            res['msg'] = "loadStatusFail"
            return click.echo(json.dumps(res))

        pre_tools = tool_status.get('tools', {})
        if pre_tools and isinstance(suite_json_data, dict):
            # add burn section to ini
            add_ret = add_burn_section(suite_json_data, project_dir, product, res, suite_json)
            
            if add_ret:
                # update status file
                tool_name = suite_json_data.get('tool_name', '')
                pre_suite_tool = pre_tools.get('suite_tool', {})
                tool_path = pre_suite_tool.get('tool_path', '')
                valid = True if tool_path else False
                suite_tool = dict(json_path=json_path, project_json_path=suite_json, product=product,
                                tool_path=tool_path, tool_name=tool_name, valid=valid)

                tool_status['tools']['suite_tool'] = suite_tool
                save_ret = update_suite_status(tool_status, status_path)
                res = {"success": save_ret}

    return click.echo(json.dumps(res))


@cli.command("set_suite_tool", short_help="set suite tool config")
@click.option(
    '-d',
    '--project-dir',
    default=os.getcwd,
    type=click.Path(
        exists=True, file_okay=False, dir_okay=True, writable=True, resolve_path=True
    )
)
@click.option("--product", "-pd", help="the product name", required=True)
@click.option('-t', '--tool-path', required=True)
def set_suite_tool(project_dir, product, tool_path):
    status_path = join(project_dir, '.deveco', 'status', f'{re.sub("[@/]", ".", product)}')
    res = {"success": False, 'msg': "setSuiteToolFail"}
    if exists(status_path):
        try:
            tool_status = json.load(open(status_path, 'r'))
        except json.decoder.JSONDecodeError:
            res['msg'] = "loadStatusFail"
            return click.echo(json.dumps(res))

        pre_tools = tool_status.get('tools', {})
        if pre_tools:
            # update status file
            pre_suite_tool = pre_tools.get('suite_tool', {})
            json_path = pre_suite_tool.get('json_path', '')
            suite_json = pre_suite_tool.get('project_json_path', '')
            tool_name = pre_suite_tool.get('tool_name', '')
            valid = True if json_path else False
            suite_tool = dict(json_path=json_path, project_json_path=suite_json, product=product,
                            tool_path=tool_path, tool_name=tool_name, valid=valid)

            tool_status['tools']['suite_tool'] = suite_tool
            update_ret = update_suite_status(tool_status, status_path)
            res = {"success": update_ret}

    return click.echo(json.dumps(res))


def add_burn_section(suite_json_data, project_dir, product, res, suite_json):
    deveco_config = ProjectConfig.get_instance(
        os.path.join(project_dir, '.deveco', 'deveco.ini')
    )
    burn_section = f"burn:{product}"
    if deveco_config.has_section(burn_section):
        deveco_config.remove_section(burn_section)
    deveco_config.add_section(burn_section)
    try:
        ui_config = suite_json_data.get('ui_config', {})
        ui_config.update(suite_json_data.get('partitions_config', {}))
        ui_config.update(suite_json_data.get('expansion_config', {}))
        for key, value in ui_config.items():
            deveco_config.set(burn_section, key, value.get('default', ''))
        deveco_config.save()
    except (ValueError, AttributeError):
        if exists(suite_json + '.backup'):
            shutil.copy(suite_json + '.backup', suite_json)
        else:
            os.unlink(suite_json)
        res['msg'] = "notSuiteJsonData"
        return False
    finally:
        if exists(suite_json + '.backup'):
            os.unlink(suite_json + '.backup')

    return True


def update_suite_status(tool_status, status_path):
    try:
        with os.fdopen(os.open(status_path, os.O_RDWR | os.O_CREAT | os.O_TRUNC, 0o640), 'w') as status_file:
            json.dump(tool_status, status_file, indent=4, ensure_ascii=False)
    except (OSError, IOError, UnicodeDecodeError):
        return False

    return True
